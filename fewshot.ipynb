{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c39113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "data = pd.read_csv('./Bengali Heatspeech dataset.csv').dropna()\n",
    "\n",
    "# Tokenize the text using XLM-Roberta tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "tokenized_data = tokenizer(\n",
    "    list(data['Text']),  # List of text samples\n",
    "    padding=True,        # Pad to the longest sequence\n",
    "    truncation=True,     # Truncate sequences longer than max_length\n",
    "    max_length=128,      # Maximum sequence length\n",
    "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Extract input IDs and attention masks\n",
    "input_ids = tokenized_data[\"input_ids\"]\n",
    "attention_masks = tokenized_data[\"attention_mask\"]\n",
    "\n",
    "# Extract labels\n",
    "labels = data[['Race', 'Behaviour', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity', 'Gender', 'Sexual Orientation', 'Political']]\n",
    "labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a7a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 02:05:08.045652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-07 02:05:08.054700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749240308.064345  376422 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749240308.067535  376422 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749240308.075653  376422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749240308.075670  376422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749240308.075672  376422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749240308.075673  376422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-07 02:05:08.079041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Training Loss: 5.108789652585983\n",
      "Epoch 2/200, Training Loss: 3.6857836320996284\n",
      "Epoch 3/200, Training Loss: 3.427482984960079\n",
      "Epoch 4/200, Training Loss: 3.308367505669594\n",
      "Epoch 5/200, Training Loss: 3.1876787170767784\n",
      "Epoch 6/200, Training Loss: 3.016574703156948\n",
      "Epoch 7/200, Training Loss: 2.9044854566454887\n",
      "Epoch 8/200, Training Loss: 2.8155576810240746\n",
      "Epoch 9/200, Training Loss: 2.7580373361706734\n",
      "Epoch 10/200, Training Loss: 2.705940544605255\n",
      "Epoch 11/200, Training Loss: 2.654635727405548\n",
      "Epoch 12/200, Training Loss: 2.5661684535443783\n",
      "Epoch 13/200, Training Loss: 2.526083506643772\n",
      "Epoch 14/200, Training Loss: 2.460718672722578\n",
      "Epoch 15/200, Training Loss: 2.425331048667431\n",
      "Epoch 16/200, Training Loss: 2.368496961891651\n",
      "Epoch 17/200, Training Loss: 2.2924506589770317\n",
      "Epoch 18/200, Training Loss: 2.2342964708805084\n",
      "Epoch 19/200, Training Loss: 2.186908580362797\n",
      "Epoch 20/200, Training Loss: 2.110852997750044\n",
      "Epoch 21/200, Training Loss: 2.0857756324112415\n",
      "Epoch 22/200, Training Loss: 1.9895709417760372\n",
      "Epoch 23/200, Training Loss: 1.924620721489191\n",
      "Epoch 24/200, Training Loss: 1.8927181847393513\n",
      "Epoch 25/200, Training Loss: 1.849560882896185\n",
      "Epoch 26/200, Training Loss: 1.7471128106117249\n",
      "Epoch 27/200, Training Loss: 1.68869986012578\n",
      "Epoch 28/200, Training Loss: 1.6521167196333408\n",
      "Epoch 29/200, Training Loss: 1.5790965929627419\n",
      "Epoch 30/200, Training Loss: 1.550413854420185\n",
      "Epoch 31/200, Training Loss: 1.5156405419111252\n",
      "Epoch 32/200, Training Loss: 1.4759513102471828\n",
      "Epoch 33/200, Training Loss: 1.3983298502862453\n",
      "Epoch 34/200, Training Loss: 1.3718194607645273\n",
      "Epoch 35/200, Training Loss: 1.335977764800191\n",
      "Epoch 36/200, Training Loss: 1.2909443825483322\n",
      "Epoch 37/200, Training Loss: 1.248019214719534\n",
      "Epoch 38/200, Training Loss: 1.2294596750289202\n",
      "Epoch 39/200, Training Loss: 1.1570886708796024\n",
      "Epoch 40/200, Training Loss: 1.1022957097738981\n",
      "Epoch 41/200, Training Loss: 1.0906081572175026\n",
      "Epoch 42/200, Training Loss: 1.0605381783097982\n",
      "Epoch 43/200, Training Loss: 0.9912924459204078\n",
      "Epoch 44/200, Training Loss: 0.9611265398561954\n",
      "Epoch 45/200, Training Loss: 0.9284975621849298\n",
      "Epoch 46/200, Training Loss: 0.9092073142528534\n",
      "Epoch 47/200, Training Loss: 0.8494078665971756\n",
      "Epoch 48/200, Training Loss: 0.814447239972651\n",
      "Epoch 49/200, Training Loss: 0.7795438658213243\n",
      "Epoch 50/200, Training Loss: 0.7114034723490477\n",
      "Warning: Loss increased or became negative! Counter: 1\n",
      "Epoch 51/200, Training Loss: 0.7210235758684576\n",
      "Epoch 52/200, Training Loss: 0.6692642369307578\n",
      "Epoch 53/200, Training Loss: 0.6309012293349952\n",
      "Epoch 54/200, Training Loss: 0.6112743695266545\n",
      "Epoch 55/200, Training Loss: 0.5632197465747595\n",
      "Epoch 56/200, Training Loss: 0.5304433992132545\n",
      "Epoch 57/200, Training Loss: 0.5161070125177503\n",
      "Epoch 58/200, Training Loss: 0.4815238956362009\n",
      "Epoch 59/200, Training Loss: 0.4591975770890713\n",
      "Epoch 60/200, Training Loss: 0.41644707415252924\n",
      "Epoch 61/200, Training Loss: 0.4040079088881612\n",
      "Epoch 62/200, Training Loss: 0.386853345669806\n",
      "Epoch 63/200, Training Loss: 0.38021682016551495\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the Multi-task Learning Model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_tasks):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.task_heads = nn.ModuleList([nn.Linear(768, 1) for _ in range(num_tasks)])  # 768 is the hidden size of XLM-Roberta\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Shared encoder output\n",
    "        shared_output = self.base_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        # Task-specific outputs\n",
    "        task_outputs = [head(shared_output) for head in self.task_heads]\n",
    "        return task_outputs\n",
    "\n",
    "\n",
    "# Load the pre-trained XLM-Roberta model\n",
    "base_model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Extract column names from the original labels DataFrame\n",
    "label_names = data[['Race', 'Behaviour', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity', 'Gender', 'Sexual Orientation', 'Political']].columns.tolist()\n",
    "\n",
    "# Number of tasks\n",
    "num_tasks = len(label_names)\n",
    "\n",
    "# Initialize the multi-task model\n",
    "multitask_model = MultiTaskModel(base_model, num_tasks)\n",
    "\n",
    "# Step 3: Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(multitask_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Step 4: Define the training loop\n",
    "# def compute_loss(outputs, labels):\n",
    "#     total_loss = 0\n",
    "#     for i, output in enumerate(outputs):\n",
    "#         total_loss += criterion(output.squeeze(), labels[:, i])\n",
    "#     return total_loss\n",
    "\n",
    "def compute_loss(outputs, labels, class_indices=None):\n",
    "    total_loss = 0\n",
    "    if class_indices is not None:\n",
    "        # Use only the outputs and labels for the specified class indices\n",
    "        outputs = [outputs[i] for i in class_indices]\n",
    "        for i, output in enumerate(outputs):\n",
    "            total_loss += criterion(output.squeeze(), labels[:, i])\n",
    "    else:\n",
    "        # General case: use all outputs and labels\n",
    "        for i, output in enumerate(outputs):\n",
    "            total_loss += criterion(output.squeeze(), labels[:, i])\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Define a threshold for stopping\n",
    "class LossMonitor:\n",
    "    def __init__(self, patience=3):\n",
    "        self.previous_loss = float('inf')  # Initialize with a very high value\n",
    "        self.patience = patience          # Number of epochs to tolerate increasing loss\n",
    "        self.counter = 0                  # Counter for consecutive increases\n",
    "\n",
    "    def check_loss(self, current_loss):\n",
    "        if current_loss > self.previous_loss or current_loss < 0:\n",
    "            self.counter += 1\n",
    "            print(f\"Warning: Loss increased or became negative! Counter: {self.counter}\")\n",
    "        else:\n",
    "            self.counter = 0  # Reset counter if loss improves\n",
    "\n",
    "        self.previous_loss = current_loss\n",
    "\n",
    "        # Stop training if loss increases for `patience` consecutive epochs\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "\n",
    "# Training the model\n",
    "epochs = 200\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "multitask_model.to(device)\n",
    "\n",
    "# Initialize the loss monitor\n",
    "loss_monitor = LossMonitor(patience=3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    multitask_model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = compute_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(multitask_model.parameters(), max_norm=1.0)  # Enable gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Check if training should stop\n",
    "    if loss_monitor.check_loss(avg_train_loss):\n",
    "        print(\"Stopping training due to increasing or invalid loss.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "multitask_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Ensure outputs are stacked correctly\n",
    "        predictions = torch.cat([torch.sigmoid(output) for output in outputs], dim=1).cpu()  # Concatenate task outputs along dimension 1\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Flatten predictions and labels\n",
    "all_predictions = torch.cat(all_predictions, dim=0)  # Concatenate along batch dimension\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Convert to NumPy for threshold tuning\n",
    "all_predictions = all_predictions.numpy()\n",
    "all_labels = all_labels.numpy()\n",
    "\n",
    "# Tune thresholds for each label\n",
    "best_thresholds = []\n",
    "for i in range(all_labels.shape[1]):  # Iterate over each label\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for threshold in np.arange(0.1, 1.0, 0.1):  # Test thresholds from 0.1 to 0.9\n",
    "        preds = (all_predictions[:, i] > threshold).astype(int)\n",
    "        f1 = f1_score(all_labels[:, i], preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "\n",
    "print(\"Best thresholds:\", best_thresholds)\n",
    "\n",
    "# Apply thresholds to predictions\n",
    "final_predictions = np.zeros_like(all_predictions)\n",
    "for i, threshold in enumerate(best_thresholds):\n",
    "    final_predictions[:, i] = (all_predictions[:, i] > threshold).astype(int)\n",
    "\n",
    "# Extract column names from the original labels DataFrame\n",
    "label_names = data[['Race', 'Behaviour', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity', 'Gender', 'Sexual Orientation', 'Political']].columns.tolist()\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(all_labels, final_predictions, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d6b75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **1. General Training**\n",
    "- **Purpose**: Train the model on all tasks (labels) using the full dataset.\n",
    "- **Outcome**:\n",
    "  - The shared base model (`XLM-Roberta`) learns general representations for all tasks.\n",
    "  - The task-specific heads are trained to predict each label, but resource-limited classes may not perform well due to insufficient data or imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Identify Low-Performing Classes**\n",
    "- After general training, evaluate the model on the validation set and analyze the **classification report**.\n",
    "- Look for:\n",
    "  - **Low F1-scores**: Indicates poor performance.\n",
    "  - **Low support**: Indicates insufficient data for the class.\n",
    "- Example (from your earlier report):\n",
    "  - `Disability`: F1-score = 0.00 (support = 11)\n",
    "  - `Physical`: F1-score = 0.21 (support = 30)\n",
    "  - `Gender`: F1-score = 0.33 (support = 45)\n",
    "  - `Sexual Orientation`: F1-score = 0.40 (support = 36)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Few-Shot Fine-Tuning**\n",
    "- **Purpose**: Focus on improving the performance of resource-limited classes.\n",
    "- **Why It Works**:\n",
    "  - The shared base model (`XLM-Roberta`) already has a strong foundation from general training.\n",
    "  - Fine-tuning on resource-limited classes allows the task-specific heads for these classes to adapt further without forgetting the general knowledge learned earlier.\n",
    "- **Steps**:\n",
    "  1. Extract a subset of the dataset containing examples for the resource-limited classes.\n",
    "  2. Fine-tune the model on this subset.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Benefits of This Approach**\n",
    "1. **Shared Knowledge**:\n",
    "   - The shared base model retains the general knowledge learned during the first phase of training.\n",
    "   - This helps the resource-limited classes benefit from the representations learned for other tasks.\n",
    "\n",
    "2. **Efficient Use of Data**:\n",
    "   - Few-shot fine-tuning focuses only on the resource-limited classes, making efficient use of the limited data available for these classes.\n",
    "\n",
    "3. **Improved Balance**:\n",
    "   - By improving the performance of low-performing classes, the overall balance across all tasks is improved.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insight**\n",
    "The **shared weights** in the base model ensure that the knowledge learned during general training is not lost. Fine-tuning only adjusts the task-specific heads and slightly refines the shared representations, making it an efficient and effective approach.\n",
    "\n",
    "Let me know if you'd like help implementing or optimizing this workflow further! 🚀\n",
    "---\n",
    "\n",
    "### **1. General Training**\n",
    "- **Purpose**: Train the model on all tasks (labels) using the full dataset.\n",
    "- **Outcome**:\n",
    "  - The shared base model (`XLM-Roberta`) learns general representations for all tasks.\n",
    "  - The task-specific heads are trained to predict each label, but resource-limited classes may not perform well due to insufficient data or imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Identify Low-Performing Classes**\n",
    "- After general training, evaluate the model on the validation set and analyze the **classification report**.\n",
    "- Look for:\n",
    "  - **Low F1-scores**: Indicates poor performance.\n",
    "  - **Low support**: Indicates insufficient data for the class.\n",
    "- Example (from your earlier report):\n",
    "  - `Disability`: F1-score = 0.00 (support = 11)\n",
    "  - `Physical`: F1-score = 0.21 (support = 30)\n",
    "  - `Gender`: F1-score = 0.33 (support = 45)\n",
    "  - `Sexual Orientation`: F1-score = 0.40 (support = 36)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Few-Shot Fine-Tuning**\n",
    "- **Purpose**: Focus on improving the performance of resource-limited classes.\n",
    "- **Why It Works**:\n",
    "  - The shared base model (`XLM-Roberta`) already has a strong foundation from general training.\n",
    "  - Fine-tuning on resource-limited classes allows the task-specific heads for these classes to adapt further without forgetting the general knowledge learned earlier.\n",
    "- **Steps**:\n",
    "  1. Extract a subset of the dataset containing examples for the resource-limited classes.\n",
    "  2. Fine-tune the model on this subset.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Benefits of This Approach**\n",
    "1. **Shared Knowledge**:\n",
    "   - The shared base model retains the general knowledge learned during the first phase of training.\n",
    "   - This helps the resource-limited classes benefit from the representations learned for other tasks.\n",
    "\n",
    "2. **Efficient Use of Data**:\n",
    "   - Few-shot fine-tuning focuses only on the resource-limited classes, making efficient use of the limited data available for these classes.\n",
    "\n",
    "3. **Improved Balance**:\n",
    "   - By improving the performance of low-performing classes, the overall balance across all tasks is improved.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insight**\n",
    "The **shared weights** in the base model ensure that the knowledge learned during general training is not lost. Fine-tuning only adjusts the task-specific heads and slightly refines the shared representations, making it an efficient and effective approach.\n",
    "\n",
    "Let me know if you'd like help implementing or optimizing this workflow further! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31584d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(outputs, labels, class_indices=None):\n",
    "    total_loss = 0\n",
    "    if class_indices is not None:\n",
    "        # Use only the outputs and labels for the specified class indices\n",
    "        outputs = [outputs[i] for i in class_indices]  # Select outputs for resource-limited classes\n",
    "        for i, output in enumerate(outputs):\n",
    "            total_loss += criterion(output.squeeze(), labels[:, i])  # Match labels to selected outputs\n",
    "    else:\n",
    "        # General case: use all outputs and labels\n",
    "        for i, output in enumerate(outputs):\n",
    "            total_loss += criterion(output.squeeze(), labels[:, i])\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dcecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify Resource-Limited Classes\n",
    "resource_limited_classes = ['Race', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity']\n",
    "\n",
    "# Step 2: Prepare Few-Shot Dataset\n",
    "# Filter the dataset for resource-limited classes\n",
    "few_shot_data = data[data[resource_limited_classes].sum(axis=1) > 0]  # Rows where at least one resource-limited class is positive\n",
    "\n",
    "# Tokenize the few-shot dataset\n",
    "few_shot_tokenized = tokenizer(\n",
    "    list(few_shot_data['Text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Extract input IDs, attention masks, and labels\n",
    "few_shot_inputs = few_shot_tokenized[\"input_ids\"]\n",
    "few_shot_masks = few_shot_tokenized[\"attention_mask\"]\n",
    "few_shot_labels = torch.tensor(few_shot_data[resource_limited_classes].values, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "few_shot_dataset = TensorDataset(few_shot_inputs, few_shot_masks, few_shot_labels)\n",
    "few_shot_dataloader = DataLoader(few_shot_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create train_data and val_data using the indices\n",
    "train_data = data.loc[train_indices]\n",
    "val_data = data.loc[val_indices]\n",
    "\n",
    "# Filter the training data for resource-limited classes\n",
    "resource_limited_classes = ['Race', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity']\n",
    "few_shot_data = train_data[train_data[resource_limited_classes].sum(axis=1) > 0]  # Use only training data\n",
    "\n",
    "# Tokenize the few-shot dataset\n",
    "few_shot_tokenized = tokenizer(\n",
    "    list(few_shot_data['Text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Extract input IDs, attention masks, and labels\n",
    "few_shot_inputs = few_shot_tokenized[\"input_ids\"]\n",
    "few_shot_masks = few_shot_tokenized[\"attention_mask\"]\n",
    "few_shot_labels = torch.tensor(few_shot_data[resource_limited_classes].values, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "few_shot_dataset = TensorDataset(few_shot_inputs, few_shot_masks, few_shot_labels)\n",
    "few_shot_dataloader = DataLoader(few_shot_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the resource-limited classes\n",
    "resource_indices = [label_names.index(cls) for cls in resource_limited_classes]\n",
    "\n",
    "# Few-Shot Fine-Tuning\n",
    "few_shot_epochs = 20\n",
    "optimizer = AdamW(multitask_model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(few_shot_epochs):\n",
    "    multitask_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in few_shot_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = compute_loss(outputs, labels, class_indices=resource_indices)  # Pass resource class indices\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(multitask_model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(few_shot_dataloader)\n",
    "        # Check if training should stop\n",
    "    if loss_monitor.check_loss(avg_train_loss):\n",
    "        print(\"Stopping training due to increasing or invalid loss.\")\n",
    "        break\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss}\")\n",
    "    print(f\"Few-Shot Epoch {epoch + 1}/{few_shot_epochs}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de480ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "multitask_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Ensure outputs are stacked correctly\n",
    "        predictions = torch.cat([torch.sigmoid(output) for output in outputs], dim=1).cpu()  # Concatenate task outputs along dimension 1\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Flatten predictions and labels\n",
    "all_predictions = torch.cat(all_predictions, dim=0)  # Concatenate along batch dimension\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Convert to NumPy for threshold tuning\n",
    "all_predictions = all_predictions.numpy()\n",
    "all_labels = all_labels.numpy()\n",
    "\n",
    "# Tune thresholds for each label\n",
    "best_thresholds = []\n",
    "for i in range(all_labels.shape[1]):  # Iterate over each label\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for threshold in np.arange(0.1, 1.0, 0.1):  # Test thresholds from 0.1 to 0.9\n",
    "        preds = (all_predictions[:, i] > threshold).astype(int)\n",
    "        f1 = f1_score(all_labels[:, i], preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "\n",
    "print(\"Best thresholds:\", best_thresholds)\n",
    "\n",
    "# Apply thresholds to predictions\n",
    "final_predictions = np.zeros_like(all_predictions)\n",
    "for i, threshold in enumerate(best_thresholds):\n",
    "    final_predictions[:, i] = (all_predictions[:, i] > threshold).astype(int)\n",
    "\n",
    "# Extract column names from the original labels DataFrame\n",
    "label_names = data[['Race', 'Behaviour', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity', 'Gender', 'Sexual Orientation', 'Political']].columns.tolist()\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(all_labels, final_predictions, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb3283",
   "metadata": {},
   "source": [
    "This classification report provides a detailed evaluation of the model's performance across multiple classes. Here's a breakdown of the metrics and their implications:\n",
    "\n",
    "---\n",
    "\n",
    "### **Metrics Explained**\n",
    "1. **Precision**:\n",
    "   - Precision measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "   - Formula: `Precision = True Positives / (True Positives + False Positives)`\n",
    "   - High precision indicates fewer false positives.\n",
    "\n",
    "2. **Recall**:\n",
    "   - Recall measures the proportion of true positive predictions out of all actual positive instances.\n",
    "   - Formula: `Recall = True Positives / (True Positives + False Negatives)`\n",
    "   - High recall indicates fewer false negatives.\n",
    "\n",
    "3. **F1-Score**:\n",
    "   - The F1-score is the harmonic mean of precision and recall, balancing both metrics.\n",
    "   - Formula: `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "   - High F1-scores indicate a good balance between precision and recall.\n",
    "\n",
    "4. **Support**:\n",
    "   - Support refers to the number of actual instances for each class in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Class-Level Analysis**\n",
    "1. **Race**:\n",
    "   - Precision: 0.71, Recall: 0.80, F1-Score: 0.75\n",
    "   - The model performs well for this class, with balanced precision and recall.\n",
    "\n",
    "2. **Behaviour**:\n",
    "   - Precision: 0.50, Recall: 0.82, F1-Score: 0.62\n",
    "   - High recall but lower precision indicates the model is good at identifying positive instances but may produce more false positives.\n",
    "\n",
    "3. **Physical**:\n",
    "   - Precision: 0.83, Recall: 1.00, F1-Score: 0.91\n",
    "   - Excellent performance with perfect recall and high precision.\n",
    "\n",
    "4. **Class**:\n",
    "   - Precision: 0.61, Recall: 0.36, F1-Score: 0.45\n",
    "   - Lower recall indicates the model struggles to identify positive instances for this class.\n",
    "\n",
    "5. **Religion**:\n",
    "   - Precision: 0.66, Recall: 0.79, F1-Score: 0.72\n",
    "   - Balanced performance with good recall and precision.\n",
    "\n",
    "6. **Disability**:\n",
    "   - Precision: 0.83, Recall: 0.91, F1-Score: 0.87\n",
    "   - Strong performance despite low support (11 samples).\n",
    "\n",
    "7. **Ethnicity**:\n",
    "   - Precision: 0.65, Recall: 0.83, F1-Score: 0.73\n",
    "   - Good recall and balanced precision.\n",
    "\n",
    "8. **Gender**:\n",
    "   - Precision: 0.68, Recall: 0.89, F1-Score: 0.77\n",
    "   - High recall and good precision indicate strong performance.\n",
    "\n",
    "9. **Sexual Orientation**:\n",
    "   - Precision: 0.80, Recall: 0.89, F1-Score: 0.84\n",
    "   - Excellent performance with high precision and recall.\n",
    "\n",
    "10. **Political**:\n",
    "    - Precision: 0.77, Recall: 0.59, F1-Score: 0.67\n",
    "    - Lower recall indicates the model misses some positive instances for this class.\n",
    "\n",
    "---\n",
    "\n",
    "### **Overall Metrics**\n",
    "1. **Micro Average**:\n",
    "   - Precision: 0.63, Recall: 0.78, F1-Score: 0.69\n",
    "   - Micro average aggregates metrics globally across all classes, treating all instances equally.\n",
    "\n",
    "2. **Macro Average**:\n",
    "   - Precision: 0.70, Recall: 0.79, F1-Score: 0.73\n",
    "   - Macro average computes metrics for each class independently and averages them, giving equal weight to all classes.\n",
    "\n",
    "3. **Weighted Average**:\n",
    "   - Precision: 0.65, Recall: 0.78, F1-Score: 0.69\n",
    "   - Weighted average considers the support of each class, giving more weight to classes with higher support.\n",
    "\n",
    "4. **Samples Average**:\n",
    "   - Precision: 0.58, Recall: 0.66, F1-Score: 0.59\n",
    "   - Samples average evaluates multi-label classification by averaging metrics across all samples.\n",
    "\n",
    "---\n",
    "\n",
    "### **Insights**\n",
    "1. **Improved Performance for Resource-Limited Classes**:\n",
    "   - Classes like `Disability`, `Physical`, `Gender`, and `Sexual Orientation` show significant improvement in precision, recall, and F1-score, likely due to the few-shot fine-tuning.\n",
    "\n",
    "2. **Balanced Performance**:\n",
    "   - The macro average F1-score (0.73) indicates balanced performance across all classes, including those with lower support.\n",
    "\n",
    "3. **Areas for Improvement**:\n",
    "   - Classes like `Class` and `Political` have lower recall, suggesting the model struggles to identify positive instances for these classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "The model demonstrates strong overall performance, particularly for resource-limited classes after few-shot fine-tuning. However, further optimization may be needed for classes with lower recall, such as `Class` and `Political`. Let me know if you'd like to explore specific improvements! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10542012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea0482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbaf0793",
   "metadata": {},
   "source": [
    "The type of **few-shot learning** used here is **fine-tuning-based few-shot learning**. This approach leverages a pre-trained model (e.g., XLM-Roberta) and fine-tunes it on a small subset of data for specific tasks or classes. Here's a breakdown of the methodology:\n",
    "\n",
    "---\n",
    "\n",
    "### **Type of Few-Shot Learning: Fine-Tuning-Based Few-Shot Learning**\n",
    "\n",
    "#### **Key Characteristics**\n",
    "1. **Pre-trained Model**:\n",
    "   - The model starts with pre-trained weights from a large dataset (e.g., XLM-Roberta trained on multilingual text).\n",
    "   - These weights provide general knowledge that can be transferred to the target tasks.\n",
    "\n",
    "2. **Fine-Tuning**:\n",
    "   - The model is fine-tuned on a small subset of labeled data for resource-limited classes.\n",
    "   - This allows the model to adapt its task-specific heads and shared representations to better handle the few-shot data.\n",
    "\n",
    "3. **Task-Specific Focus**:\n",
    "   - Instead of retraining the model on all tasks, the fine-tuning focuses only on the resource-limited classes (e.g., `Disability`, `Physical`, `Gender`, `Sexual Orientation`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Fine-Tuning-Based Few-Shot Learning?**\n",
    "This approach is suitable because:\n",
    "1. **Pre-trained Knowledge**:\n",
    "   - The shared base model (XLM-Roberta) already has general knowledge from the initial training phase.\n",
    "   - Fine-tuning refines this knowledge for specific tasks or classes.\n",
    "\n",
    "2. **Small Data Availability**:\n",
    "   - Few-shot learning is ideal for resource-limited classes with low support (e.g., `Disability` with only 11 samples).\n",
    "\n",
    "3. **Efficiency**:\n",
    "   - Fine-tuning only adjusts the task-specific heads and slightly refines the shared representations, making it computationally efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### **Alternative Few-Shot Learning Approaches**\n",
    "If you want to explore other few-shot learning paradigms, here are some alternatives:\n",
    "\n",
    "#### **1. Meta-Learning-Based Few-Shot Learning**\n",
    "- **Example**: **MAML (Model-Agnostic Meta-Learning)** or **Prototypical Networks**.\n",
    "- **How It Works**:\n",
    "  - The model is trained to quickly adapt to new tasks with minimal data.\n",
    "  - Instead of fine-tuning, the model learns a meta-knowledge representation that generalizes across tasks.\n",
    "- **Use Case**:\n",
    "  - Ideal for scenarios where the model needs to adapt to entirely new tasks or classes.\n",
    "\n",
    "#### **2. Prompt-Based Few-Shot Learning**\n",
    "- **Example**: GPT-style models with in-context learning.\n",
    "- **How It Works**:\n",
    "  - The model is provided with a few examples in the input prompt (e.g., \"Here are 3 examples of class X\").\n",
    "  - No fine-tuning is required; the model uses its pre-trained knowledge to infer the task.\n",
    "- **Use Case**:\n",
    "  - Suitable for large language models with extensive pre-training.\n",
    "\n",
    "#### **3. Augmentation-Based Few-Shot Learning**\n",
    "- **How It Works**:\n",
    "  - Augment the few-shot dataset using techniques like paraphrasing, back-translation, or adding noise.\n",
    "  - Train the model on the augmented dataset to improve generalization.\n",
    "- **Use Case**:\n",
    "  - Useful when the few-shot dataset is extremely small.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Fine-Tuning Was Chosen Here**\n",
    "- **Pre-trained Model**: XLM-Roberta is already trained on a large multilingual corpus, making it ideal for transfer learning.\n",
    "- **Resource-Limited Classes**: Fine-tuning allows targeted improvement for specific classes without retraining the entire model.\n",
    "- **Efficiency**: Fine-tuning is computationally efficient compared to meta-learning or prompt-based approaches.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like to explore other few-shot learning paradigms or need further clarification! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce87823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 02:17:40.850819: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-07 02:17:40.863944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749241060.877585  387582 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749241060.881638  387582 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749241060.893670  387582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749241060.893688  387582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749241060.893690  387582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749241060.893691  387582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-07 02:17:40.898444: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 6.2243523597717285\n",
      "Epoch 2/50, Training Loss: 3.790535628795624\n",
      "Epoch 3/50, Training Loss: 3.467610940337181\n",
      "Epoch 4/50, Training Loss: 3.343405559659004\n",
      "Epoch 5/50, Training Loss: 3.1992236226797104\n",
      "Epoch 6/50, Training Loss: 3.085482895374298\n",
      "Epoch 7/50, Training Loss: 2.955268256366253\n",
      "Epoch 8/50, Training Loss: 2.8634586110711098\n",
      "Epoch 9/50, Training Loss: 2.79030305147171\n",
      "Epoch 10/50, Training Loss: 2.7488602995872498\n",
      "Epoch 11/50, Training Loss: 2.6752632558345795\n",
      "Epoch 12/50, Training Loss: 2.643180102109909\n",
      "Epoch 13/50, Training Loss: 2.5848512649536133\n",
      "Epoch 14/50, Training Loss: 2.5357800610363483\n",
      "Epoch 15/50, Training Loss: 2.4633786976337433\n",
      "Epoch 16/50, Training Loss: 2.411793239414692\n",
      "Epoch 17/50, Training Loss: 2.3641993403434753\n",
      "Epoch 18/50, Training Loss: 2.3147896640002728\n",
      "Epoch 19/50, Training Loss: 2.2783000953495502\n",
      "Epoch 20/50, Training Loss: 2.218956932425499\n",
      "Epoch 21/50, Training Loss: 2.136879589408636\n",
      "Epoch 22/50, Training Loss: 2.1055227033793926\n",
      "Epoch 23/50, Training Loss: 2.032403949648142\n",
      "Epoch 24/50, Training Loss: 2.0103502981364727\n",
      "Epoch 25/50, Training Loss: 1.9738942608237267\n",
      "Epoch 26/50, Training Loss: 1.870638757944107\n",
      "Epoch 27/50, Training Loss: 1.8449297957122326\n",
      "Epoch 28/50, Training Loss: 1.7686531208455563\n",
      "Epoch 29/50, Training Loss: 1.7253178022801876\n",
      "Epoch 30/50, Training Loss: 1.6801372729241848\n",
      "Epoch 31/50, Training Loss: 1.626068003475666\n",
      "Epoch 32/50, Training Loss: 1.58823062479496\n",
      "Epoch 33/50, Training Loss: 1.5059243999421597\n",
      "Epoch 34/50, Training Loss: 1.46732934191823\n",
      "Epoch 35/50, Training Loss: 1.4270280413329601\n",
      "Epoch 36/50, Training Loss: 1.4002662487328053\n",
      "Epoch 37/50, Training Loss: 1.332157015800476\n",
      "Epoch 38/50, Training Loss: 1.2770211454480886\n",
      "Epoch 39/50, Training Loss: 1.2350983321666718\n",
      "Epoch 40/50, Training Loss: 1.1958661898970604\n",
      "Epoch 41/50, Training Loss: 1.1569276545196772\n",
      "Epoch 42/50, Training Loss: 1.1083302237093449\n",
      "Epoch 43/50, Training Loss: 1.079992562532425\n",
      "Epoch 44/50, Training Loss: 1.0330312680453062\n",
      "Epoch 45/50, Training Loss: 1.0233483705669641\n",
      "Epoch 46/50, Training Loss: 0.9947966518811882\n",
      "Epoch 47/50, Training Loss: 0.9453567434102297\n",
      "Epoch 48/50, Training Loss: 0.8997171623632312\n",
      "Epoch 49/50, Training Loss: 0.8972511189058423\n",
      "Epoch 50/50, Training Loss: 0.845072123222053\n",
      "Best thresholds: [np.float64(0.1), np.float64(0.30000000000000004), np.float64(0.30000000000000004), np.float64(0.4), np.float64(0.30000000000000004), 0.5, np.float64(0.1), np.float64(0.4), np.float64(0.5), np.float64(0.1)]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.35      0.39      0.37        64\n",
      "         Behaviour       0.54      0.76      0.63       194\n",
      "          Physical       0.25      0.20      0.22        30\n",
      "             Class       0.40      0.21      0.27        39\n",
      "          Religion       0.62      0.47      0.54        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.48      0.60      0.53       110\n",
      "            Gender       0.49      0.40      0.44        45\n",
      "Sexual Orientation       0.56      0.39      0.46        36\n",
      "         Political       0.65      0.69      0.67       106\n",
      "\n",
      "         micro avg       0.52      0.55      0.54       716\n",
      "         macro avg       0.43      0.41      0.41       716\n",
      "      weighted avg       0.51      0.55      0.52       716\n",
      "       samples avg       0.46      0.48      0.44       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('./Bengali Heatspeech dataset.csv').dropna()\n",
    "\n",
    "# Step 2: Dynamically set the number of tasks\n",
    "label_columns = ['Race', 'Behaviour', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity', 'Gender', 'Sexual Orientation', 'Political']\n",
    "num_tasks = len(label_columns)\n",
    "\n",
    "# Step 3: Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Tokenize the data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Tokenize training data\n",
    "train_tokenized = tokenizer(\n",
    "    list(train_data['Text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Tokenize validation data\n",
    "val_tokenized = tokenizer(\n",
    "    list(val_data['Text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Extract input IDs, attention masks, and labels for training and validation data\n",
    "train_inputs = train_tokenized[\"input_ids\"]\n",
    "train_masks = train_tokenized[\"attention_mask\"]\n",
    "train_labels = torch.tensor(train_data[label_columns].values, dtype=torch.float32)\n",
    "\n",
    "val_inputs = val_tokenized[\"input_ids\"]\n",
    "val_masks = val_tokenized[\"attention_mask\"]\n",
    "val_labels = torch.tensor(val_data[label_columns].values, dtype=torch.float32)\n",
    "\n",
    "# Step 5: Create DataLoaders\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Step 6: Define the Multi-task Learning Model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_tasks):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.task_heads = nn.ModuleList([nn.Linear(768, 1) for _ in range(num_tasks)])  # 768 is the hidden size of XLM-Roberta\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        shared_output = self.base_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        task_outputs = [head(shared_output) for head in self.task_heads]\n",
    "        return task_outputs\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "multitask_model = MultiTaskModel(base_model, num_tasks)\n",
    "\n",
    "# Step 7: Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(multitask_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Step 8: Training Loop\n",
    "epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "multitask_model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    multitask_model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Debugging: Print shapes\n",
    "        # print(f\"Number of outputs: {len(outputs)}\")\n",
    "        # print(f\"Labels shape: {labels.shape}\")\n",
    "        # Ensure outputs and labels are aligned\n",
    "        assert len(outputs) == labels.shape[1], \"Mismatch between model outputs and labels\"\n",
    "        loss = sum(criterion(output.squeeze(), labels[:, i]) for i, output in enumerate(outputs))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(multitask_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss}\")\n",
    "\n",
    "# Step 9: Evaluate the Model\n",
    "multitask_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.cat([torch.sigmoid(output) for output in outputs], dim=1).cpu()\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "all_predictions = torch.cat(all_predictions, dim=0).numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "best_thresholds = []\n",
    "for i in range(all_labels.shape[1]):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for threshold in np.arange(0.1, 1.0, 0.1):\n",
    "        preds = (all_predictions[:, i] > threshold).astype(int)\n",
    "        f1 = f1_score(all_labels[:, i], preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "\n",
    "print(\"Best thresholds:\", best_thresholds)\n",
    "\n",
    "final_predictions = np.zeros_like(all_predictions)\n",
    "for i, threshold in enumerate(best_thresholds):\n",
    "    final_predictions[:, i] = (all_predictions[:, i] > threshold).astype(int)\n",
    "\n",
    "print(classification_report(all_labels, final_predictions, target_names=label_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "# Step 1: Generate predictions and labels\n",
    "multitask_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.cat([torch.sigmoid(output) for output in outputs], dim=1).cpu()\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Flatten predictions and labels\n",
    "all_predictions = torch.cat(all_predictions, dim=0).numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "# Step 2: Tune thresholds for each class\n",
    "best_thresholds = []\n",
    "for i in range(all_labels.shape[1]):  # Iterate over each class\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for threshold in np.arange(0.1, 1.0, 0.1):  # Test thresholds from 0.1 to 0.9\n",
    "        preds = (all_predictions[:, i] > threshold).astype(int)\n",
    "        f1 = f1_score(all_labels[:, i], preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "\n",
    "print(\"Best thresholds:\", best_thresholds)\n",
    "\n",
    "# Step 3: Apply thresholds to predictions\n",
    "final_predictions = np.zeros_like(all_predictions)\n",
    "for i, threshold in enumerate(best_thresholds):\n",
    "    final_predictions[:, i] = (all_predictions[:, i] > threshold).astype(int)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(all_labels, final_predictions, target_names=label_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0fac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Epoch 1/5, Loss: 1.0140941497825442\n",
      "Few-Shot Epoch 2/5, Loss: 0.990690614212127\n",
      "Few-Shot Epoch 3/5, Loss: 0.9603790286041441\n",
      "Few-Shot Epoch 4/5, Loss: 0.9416010152725947\n",
      "Few-Shot Epoch 5/5, Loss: 0.9176006478922708\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify Resource-Limited Classes\n",
    "resource_limited_classes = ['Race', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity']\n",
    "\n",
    "# Step 2: Map Resource-Limited Classes to Indices\n",
    "resource_indices = [label_columns.index(cls) for cls in resource_limited_classes]\n",
    "\n",
    "# Step 3: Filter Few-Shot Data from Training Data\n",
    "few_shot_data = train_data[train_data[resource_limited_classes].sum(axis=1) > 0]\n",
    "\n",
    "# Step 4: Tokenize the Few-Shot Dataset\n",
    "few_shot_tokenized = tokenizer(\n",
    "    list(few_shot_data['Text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "few_shot_inputs = few_shot_tokenized[\"input_ids\"]\n",
    "few_shot_masks = few_shot_tokenized[\"attention_mask\"]\n",
    "few_shot_labels = torch.tensor(few_shot_data[resource_limited_classes].values, dtype=torch.float32)\n",
    "\n",
    "# Step 5: Create Few-Shot DataLoader\n",
    "few_shot_dataset = TensorDataset(few_shot_inputs, few_shot_masks, few_shot_labels)\n",
    "few_shot_dataloader = DataLoader(few_shot_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "for param in multitask_model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Step 6: Fine-Tune the Model\n",
    "few_shot_epochs = 20\n",
    "optimizer = AdamW(multitask_model.parameters(), lr=1e-5)\n",
    "\n",
    "few_shot_epochs = 5\n",
    "for epoch in range(few_shot_epochs):\n",
    "    multitask_model.train()\n",
    "    total_loss = 0\n",
    "    for batch in few_shot_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Compute loss only for resource-limited classes\n",
    "        loss = sum(criterion(outputs[idx].squeeze(), labels[:, i]) for i, idx in enumerate(resource_indices))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(multitask_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(few_shot_dataloader)\n",
    "    print(f\"Few-Shot Epoch {epoch + 1}/{few_shot_epochs}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb61b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best thresholds: [np.float64(0.1), np.float64(0.30000000000000004), np.float64(0.30000000000000004), np.float64(0.4), np.float64(0.1), 0.5, np.float64(0.1), np.float64(0.4), np.float64(0.5), np.float64(0.1)]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.32      0.48      0.39        64\n",
      "         Behaviour       0.54      0.76      0.63       194\n",
      "          Physical       0.20      0.23      0.22        30\n",
      "             Class       0.38      0.21      0.27        39\n",
      "          Religion       0.50      0.65      0.57        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.47      0.62      0.54       110\n",
      "            Gender       0.49      0.40      0.44        45\n",
      "Sexual Orientation       0.56      0.39      0.46        36\n",
      "         Political       0.65      0.69      0.67       106\n",
      "\n",
      "         micro avg       0.49      0.59      0.54       716\n",
      "         macro avg       0.41      0.44      0.42       716\n",
      "      weighted avg       0.49      0.59      0.53       716\n",
      "       samples avg       0.45      0.50      0.45       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Step 1: Generate predictions and labels\n",
    "multitask_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = multitask_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.cat([torch.sigmoid(output) for output in outputs], dim=1).cpu()\n",
    "        all_predictions.append(predictions)\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Flatten predictions and labels\n",
    "all_predictions = torch.cat(all_predictions, dim=0).numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "# Step 2: Tune thresholds for each class\n",
    "best_thresholds = []\n",
    "for i in range(all_labels.shape[1]):  # Iterate over each class\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for threshold in np.arange(0.1, 1.0, 0.1):  # Test thresholds from 0.1 to 0.9\n",
    "        preds = (all_predictions[:, i] > threshold).astype(int)\n",
    "        f1 = f1_score(all_labels[:, i], preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "\n",
    "print(\"Best thresholds:\", best_thresholds)\n",
    "\n",
    "# Step 3: Apply thresholds to predictions\n",
    "final_predictions = np.zeros_like(all_predictions)\n",
    "for i, threshold in enumerate(best_thresholds):\n",
    "    final_predictions[:, i] = (all_predictions[:, i] > threshold).astype(int)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(all_labels, final_predictions, target_names=label_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e5991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 02:48:42.993667: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-07 02:48:43.079798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749242923.112890    2520 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749242923.123095    2520 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749242923.195588    2520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749242923.195607    2520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749242923.195608    2520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749242923.195609    2520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-07 02:48:43.205131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial training...\n",
      "Epoch 1, Loss: 5.629242122173309\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.13      1.00      0.23        64\n",
      "         Behaviour       0.39      1.00      0.56       194\n",
      "          Physical       0.06      0.77      0.12        30\n",
      "             Class       0.06      0.33      0.11        39\n",
      "          Religion       0.16      1.00      0.28        81\n",
      "        Disability       0.02      0.36      0.04        11\n",
      "         Ethnicity       0.22      1.00      0.36       110\n",
      "            Gender       0.09      0.89      0.16        45\n",
      "Sexual Orientation       0.08      1.00      0.15        36\n",
      "         Political       0.21      1.00      0.35       106\n",
      "\n",
      "         micro avg       0.16      0.94      0.27       716\n",
      "         macro avg       0.14      0.84      0.23       716\n",
      "      weighted avg       0.22      0.94      0.34       716\n",
      "       samples avg       0.16      0.78      0.26       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 3.6963652819395065\n",
      "Epoch 3, Loss: 3.4588871747255325\n",
      "Epoch 4, Loss: 3.3180352449417114\n",
      "Epoch 5, Loss: 3.228292889893055\n",
      "Epoch 6, Loss: 3.126803159713745\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.32      0.75      0.45        64\n",
      "         Behaviour       0.48      0.81      0.60       194\n",
      "          Physical       0.15      0.10      0.12        30\n",
      "             Class       0.10      0.62      0.17        39\n",
      "          Religion       0.33      0.58      0.42        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.46      0.66      0.54       110\n",
      "            Gender       0.24      0.44      0.31        45\n",
      "Sexual Orientation       0.29      0.75      0.42        36\n",
      "         Political       0.55      0.75      0.63       106\n",
      "\n",
      "         micro avg       0.35      0.67      0.46       716\n",
      "         macro avg       0.29      0.55      0.37       716\n",
      "      weighted avg       0.39      0.67      0.48       716\n",
      "       samples avg       0.38      0.56      0.41       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 3.037480317056179\n",
      "Epoch 8, Loss: 2.9275747388601303\n",
      "Epoch 9, Loss: 2.8551408275961876\n",
      "Epoch 10, Loss: 2.7899747267365456\n",
      "Epoch 11, Loss: 2.7150727435946465\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.37      0.64      0.47        64\n",
      "         Behaviour       0.56      0.70      0.62       194\n",
      "          Physical       0.17      0.27      0.21        30\n",
      "             Class       0.21      0.41      0.28        39\n",
      "          Religion       0.58      0.52      0.55        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.53      0.74      0.62       110\n",
      "            Gender       0.26      0.64      0.37        45\n",
      "Sexual Orientation       0.37      0.47      0.41        36\n",
      "         Political       0.63      0.72      0.67       106\n",
      "\n",
      "         micro avg       0.46      0.62      0.53       716\n",
      "         macro avg       0.37      0.51      0.42       716\n",
      "      weighted avg       0.48      0.62      0.53       716\n",
      "       samples avg       0.46      0.51      0.45       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 2.624038189649582\n",
      "Epoch 13, Loss: 2.5840100087225437\n",
      "Epoch 14, Loss: 2.55990132689476\n",
      "Epoch 15, Loss: 2.5104625895619392\n",
      "Epoch 16, Loss: 2.4391228035092354\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.48      0.45      0.46        64\n",
      "         Behaviour       0.53      0.76      0.62       194\n",
      "          Physical       0.12      0.10      0.11        30\n",
      "             Class       0.29      0.26      0.27        39\n",
      "          Religion       0.52      0.63      0.57        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.53      0.77      0.63       110\n",
      "            Gender       0.29      0.51      0.37        45\n",
      "Sexual Orientation       0.34      0.53      0.41        36\n",
      "         Political       0.70      0.70      0.70       106\n",
      "\n",
      "         micro avg       0.49      0.62      0.55       716\n",
      "         macro avg       0.38      0.47      0.41       716\n",
      "      weighted avg       0.49      0.62      0.54       716\n",
      "       samples avg       0.50      0.53      0.47       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 2.387563604861498\n",
      "Epoch 18, Loss: 2.3072171807289124\n",
      "Epoch 19, Loss: 2.271461881697178\n",
      "Epoch 20, Loss: 2.1843342669308186\n",
      "Epoch 21, Loss: 2.123511478304863\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.37      0.55      0.44        64\n",
      "         Behaviour       0.63      0.65      0.64       194\n",
      "          Physical       0.20      0.30      0.24        30\n",
      "             Class       0.34      0.38      0.36        39\n",
      "          Religion       0.53      0.64      0.58        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.52      0.70      0.60       110\n",
      "            Gender       0.28      0.51      0.36        45\n",
      "Sexual Orientation       0.35      0.58      0.44        36\n",
      "         Political       0.62      0.68      0.65       106\n",
      "\n",
      "         micro avg       0.48      0.60      0.54       716\n",
      "         macro avg       0.38      0.50      0.43       716\n",
      "      weighted avg       0.50      0.60      0.54       716\n",
      "       samples avg       0.45      0.51      0.44       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 2.112840037792921\n",
      "Epoch 23, Loss: 2.0486152209341526\n",
      "Epoch 24, Loss: 1.9969901219010353\n",
      "Epoch 25, Loss: 1.9157525785267353\n",
      "Epoch 26, Loss: 1.8659468851983547\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.39      0.48      0.43        64\n",
      "         Behaviour       0.58      0.67      0.62       194\n",
      "          Physical       0.22      0.23      0.23        30\n",
      "             Class       0.30      0.28      0.29        39\n",
      "          Religion       0.48      0.77      0.59        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.51      0.69      0.58       110\n",
      "            Gender       0.30      0.56      0.39        45\n",
      "Sexual Orientation       0.37      0.61      0.46        36\n",
      "         Political       0.65      0.67      0.66       106\n",
      "\n",
      "         micro avg       0.48      0.61      0.54       716\n",
      "         macro avg       0.38      0.50      0.43       716\n",
      "      weighted avg       0.48      0.61      0.54       716\n",
      "       samples avg       0.45      0.52      0.45       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 1.8442133963108063\n",
      "Epoch 28, Loss: 1.7935220710933208\n",
      "Epoch 29, Loss: 1.7498814798891544\n",
      "Epoch 30, Loss: 1.6849523559212685\n",
      "Epoch 31, Loss: 1.6454904302954674\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.40      0.53      0.46        64\n",
      "         Behaviour       0.46      0.85      0.60       194\n",
      "          Physical       0.16      0.23      0.19        30\n",
      "             Class       0.21      0.38      0.27        39\n",
      "          Religion       0.48      0.75      0.59        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.48      0.65      0.55       110\n",
      "            Gender       0.58      0.31      0.41        45\n",
      "Sexual Orientation       0.38      0.56      0.45        36\n",
      "         Political       0.58      0.74      0.65       106\n",
      "\n",
      "         micro avg       0.44      0.65      0.53       716\n",
      "         macro avg       0.37      0.50      0.42       716\n",
      "      weighted avg       0.45      0.65      0.52       716\n",
      "       samples avg       0.44      0.55      0.46       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 1.5688375011086464\n",
      "Epoch 33, Loss: 1.5530081205070019\n",
      "Epoch 34, Loss: 1.455374775454402\n",
      "Epoch 35, Loss: 1.4524581916630268\n",
      "Epoch 36, Loss: 1.3636515624821186\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.41      0.45      0.43        64\n",
      "         Behaviour       0.49      0.74      0.59       194\n",
      "          Physical       0.15      0.27      0.19        30\n",
      "             Class       0.35      0.18      0.24        39\n",
      "          Religion       0.53      0.62      0.57        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.46      0.61      0.53       110\n",
      "            Gender       0.39      0.49      0.43        45\n",
      "Sexual Orientation       0.52      0.47      0.49        36\n",
      "         Political       0.70      0.61      0.65       106\n",
      "\n",
      "         micro avg       0.48      0.57      0.52       716\n",
      "         macro avg       0.40      0.44      0.41       716\n",
      "      weighted avg       0.48      0.57      0.51       716\n",
      "       samples avg       0.43      0.50      0.43       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 1.327190676704049\n",
      "Epoch 38, Loss: 1.2679235581308603\n",
      "Epoch 39, Loss: 1.2250130465254188\n",
      "Epoch 40, Loss: 1.1855799742043018\n",
      "Epoch 41, Loss: 1.1420283475890756\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.37      0.50      0.43        64\n",
      "         Behaviour       0.58      0.62      0.60       194\n",
      "          Physical       0.15      0.27      0.19        30\n",
      "             Class       0.23      0.33      0.27        39\n",
      "          Religion       0.51      0.69      0.59        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.48      0.60      0.53       110\n",
      "            Gender       0.45      0.40      0.42        45\n",
      "Sexual Orientation       0.50      0.56      0.53        36\n",
      "         Political       0.56      0.68      0.62       106\n",
      "\n",
      "         micro avg       0.47      0.57      0.51       716\n",
      "         macro avg       0.38      0.46      0.42       716\n",
      "      weighted avg       0.48      0.57      0.52       716\n",
      "       samples avg       0.42      0.49      0.42       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: 1.092144774273038\n",
      "Epoch 43, Loss: 1.0658020144328475\n",
      "Epoch 44, Loss: 1.0514668114483356\n",
      "Epoch 45, Loss: 0.9753288077190518\n",
      "Epoch 46, Loss: 0.9702897761017084\n",
      "\n",
      "Validation Performance:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.30      0.53      0.39        64\n",
      "         Behaviour       0.51      0.77      0.62       194\n",
      "          Physical       0.13      0.30      0.19        30\n",
      "             Class       0.40      0.21      0.27        39\n",
      "          Religion       0.50      0.68      0.58        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.48      0.53      0.50       110\n",
      "            Gender       0.41      0.42      0.42        45\n",
      "Sexual Orientation       0.40      0.56      0.47        36\n",
      "         Political       0.62      0.70      0.65       106\n",
      "\n",
      "         micro avg       0.45      0.60      0.52       716\n",
      "         macro avg       0.38      0.47      0.41       716\n",
      "      weighted avg       0.46      0.60      0.51       716\n",
      "       samples avg       0.43      0.52      0.44       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: 0.9070147937163711\n",
      "Epoch 48, Loss: 0.8517710326705128\n",
      "Epoch 49, Loss: 0.8309390826616436\n",
      "Epoch 50, Loss: 0.7831956269219518\n",
      "\n",
      "Starting few-shot fine-tuning...\n",
      "Few-shot Epoch 1, Loss: 0.009012974544841432\n",
      "Few-shot Epoch 2, Loss: 0.0076876540188642595\n",
      "Few-shot Epoch 3, Loss: 0.006970446816771606\n",
      "Few-shot Epoch 4, Loss: 0.005417284438442579\n",
      "Few-shot Epoch 5, Loss: 0.0046823401217649975\n",
      "\n",
      "Final Evaluation:\n",
      "\n",
      "Best thresholds: [np.float64(0.7000000000000001), np.float64(0.1), np.float64(0.9), np.float64(0.9), np.float64(0.6), np.float64(0.9), np.float64(0.5), np.float64(0.1), np.float64(0.2), np.float64(0.30000000000000004)]\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.68      0.77      0.72        64\n",
      "         Behaviour       0.45      0.77      0.57       194\n",
      "          Physical       0.50      0.60      0.55        30\n",
      "             Class       0.84      0.41      0.55        39\n",
      "          Religion       0.68      0.78      0.73        81\n",
      "        Disability       1.00      0.64      0.78        11\n",
      "         Ethnicity       0.59      0.69      0.64       110\n",
      "            Gender       0.39      0.62      0.48        45\n",
      "Sexual Orientation       0.38      0.67      0.48        36\n",
      "         Political       0.58      0.71      0.64       106\n",
      "\n",
      "         micro avg       0.53      0.71      0.61       716\n",
      "         macro avg       0.61      0.66      0.61       716\n",
      "      weighted avg       0.56      0.71      0.61       716\n",
      "       samples avg       0.49      0.60      0.51       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_data(file_path, label_columns):\n",
    "    \"\"\"Load and preprocess the dataset\"\"\"\n",
    "    data = pd.read_csv(file_path).dropna()\n",
    "    labels = torch.tensor(data[label_columns].values, dtype=torch.float32)\n",
    "    return data, labels\n",
    "\n",
    "def tokenize_data(texts, tokenizer, max_length=128):\n",
    "    \"\"\"Tokenize the input texts\"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "def create_dataloaders(input_ids, attention_masks, labels, batch_size=64, test_size=0.2):\n",
    "    \"\"\"Create train and validation dataloaders\"\"\"\n",
    "    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "        input_ids, attention_masks, labels, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, (train_labels, val_labels)\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_tasks):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.task_heads = nn.ModuleList([nn.Linear(768, 1) for _ in range(num_tasks)])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        shared_output = self.base_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        task_outputs = [head(shared_output) for head in self.task_heads]\n",
    "        return task_outputs\n",
    "\n",
    "def compute_loss(outputs, labels, criterion, class_indices=None, class_weights=None):\n",
    "    \"\"\"Compute the loss with optional class weights and indices\"\"\"\n",
    "    total_loss = 0\n",
    "    if class_indices is not None:\n",
    "        for i, idx in enumerate(class_indices):\n",
    "            weight = class_weights[i] if class_weights is not None else 1.0\n",
    "            total_loss += weight * criterion(outputs[idx].squeeze(), labels[:, i])\n",
    "    else:\n",
    "        for i, output in enumerate(outputs):\n",
    "            total_loss += criterion(output.squeeze(), labels[:, i])\n",
    "    return total_loss\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, class_indices=None, class_weights=None):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = compute_loss(outputs, labels, criterion, class_indices, class_weights)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, device, label_names):\n",
    "    \"\"\"Evaluate the model and return predictions and metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.cat([torch.sigmoid(output) for output in outputs], dim=1).cpu()\n",
    "            all_predictions.append(predictions)\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_predictions = torch.cat(all_predictions, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    # Find best thresholds\n",
    "    best_thresholds = []\n",
    "    for i in range(all_labels.shape[1]):\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        for threshold in np.arange(0.1, 1.0, 0.1):\n",
    "            preds = (all_predictions[:, i] > threshold).astype(int)\n",
    "            f1 = f1_score(all_labels[:, i], preds)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        best_thresholds.append(best_threshold)\n",
    "    \n",
    "    # Apply thresholds\n",
    "    final_predictions = np.zeros_like(all_predictions)\n",
    "    for i, threshold in enumerate(best_thresholds):\n",
    "        final_predictions[:, i] = (all_predictions[:, i] > threshold).astype(int)\n",
    "    \n",
    "    return final_predictions, all_labels, best_thresholds\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    label_columns = ['Race', 'Behaviour', 'Physical', 'Class', 'Religion', \n",
    "                    'Disability', 'Ethnicity', 'Gender', 'Sexual Orientation', 'Political']\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    data, labels = load_and_preprocess_data('./Bengali Heatspeech dataset.csv', label_columns)\n",
    "    \n",
    "    # Initialize tokenizer and tokenize data\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    tokenized = tokenize_data(list(data['Text']), tokenizer)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader, val_dataloader, (train_labels, val_labels) = create_dataloaders(\n",
    "        tokenized[\"input_ids\"],\n",
    "        tokenized[\"attention_mask\"],\n",
    "        labels\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    base_model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "    model = MultiTaskModel(base_model, len(label_columns)).to(device)\n",
    "    \n",
    "    # Training configuration\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    # Initial training\n",
    "    print(\"Starting initial training...\")\n",
    "    for epoch in range(50):\n",
    "        avg_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
    "        \n",
    "        if epoch % 5 == 0:  # Evaluate every 5 epochs\n",
    "            predictions, labels, thresholds = evaluate_model(model, val_dataloader, device, label_columns)\n",
    "            print(\"\\nValidation Performance:\")\n",
    "            print(classification_report(labels, predictions, target_names=label_columns))\n",
    "    \n",
    "    # Few-shot fine-tuning\n",
    "    print(\"\\nStarting few-shot fine-tuning...\")\n",
    "    resource_limited_classes = ['Race', 'Physical', 'Class', 'Religion', 'Disability', 'Ethnicity']\n",
    "    resource_indices = [label_columns.index(cls) for cls in resource_limited_classes]\n",
    "    \n",
    "    # Prepare few-shot data\n",
    "    few_shot_data = data[data[resource_limited_classes].sum(axis=1) > 0]\n",
    "    few_shot_tokenized = tokenize_data(list(few_shot_data['Text']), tokenizer)\n",
    "    few_shot_labels = torch.tensor(few_shot_data[resource_limited_classes].values, dtype=torch.float32)\n",
    "    \n",
    "    few_shot_dataset = TensorDataset(\n",
    "        few_shot_tokenized[\"input_ids\"],\n",
    "        few_shot_tokenized[\"attention_mask\"],\n",
    "        few_shot_labels\n",
    "    )\n",
    "    few_shot_dataloader = DataLoader(few_shot_dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    # Calculate class weights for few-shot learning\n",
    "    class_counts = few_shot_data[resource_limited_classes].sum(axis=0)\n",
    "    class_weights = torch.tensor([1.0 / count if count > 0 else 1.0 for count in class_counts], \n",
    "                               dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Fine-tune\n",
    "    for epoch in range(5):\n",
    "        avg_loss = train_epoch(\n",
    "            model, \n",
    "            few_shot_dataloader, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            device, \n",
    "            class_indices=resource_indices,\n",
    "            class_weights=class_weights\n",
    "        )\n",
    "        print(f\"Few-shot Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    predictions, labels, thresholds = evaluate_model(model, val_dataloader, device, label_columns)\n",
    "    print(\"\\nBest thresholds:\", thresholds)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predictions, target_names=label_columns))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499fba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55, Training Loss: 0.4458373368732513\n",
      "Epoch 2/55, Training Loss: 0.34477542459018645\n",
      "Validation Loss: 0.3423271421343088\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.00      0.00      0.00        64\n",
      "         Behaviour       0.58      0.54      0.56       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.00      0.00      0.00        39\n",
      "          Religion       0.00      0.00      0.00        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.00      0.00      0.00       110\n",
      "            Gender       0.00      0.00      0.00        45\n",
      "Sexual Orientation       0.00      0.00      0.00        36\n",
      "         Political       0.54      0.26      0.35       106\n",
      "\n",
      "         micro avg       0.57      0.19      0.28       716\n",
      "         macro avg       0.11      0.08      0.09       716\n",
      "      weighted avg       0.24      0.19      0.20       716\n",
      "       samples avg       0.27      0.19      0.21       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/55, Training Loss: 0.31791016837907216\n",
      "Epoch 4/55, Training Loss: 0.2944117555069545\n",
      "Validation Loss: 0.29346819780766964\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.50      0.02      0.03        64\n",
      "         Behaviour       0.61      0.51      0.56       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.00      0.00      0.00        39\n",
      "          Religion       0.64      0.51      0.57        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.57      0.38      0.46       110\n",
      "            Gender       0.55      0.13      0.21        45\n",
      "Sexual Orientation       0.00      0.00      0.00        36\n",
      "         Political       0.75      0.66      0.70       106\n",
      "\n",
      "         micro avg       0.64      0.36      0.46       716\n",
      "         macro avg       0.36      0.22      0.25       716\n",
      "      weighted avg       0.52      0.36      0.41       716\n",
      "       samples avg       0.49      0.35      0.39       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/55, Training Loss: 0.27701051556874834\n",
      "Epoch 6/55, Training Loss: 0.2703271082469395\n",
      "Validation Loss: 0.29967109579592943\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.00      0.00      0.00        64\n",
      "         Behaviour       0.64      0.45      0.53       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.00      0.00      0.00        39\n",
      "          Religion       0.72      0.41      0.52        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.66      0.21      0.32       110\n",
      "            Gender       0.47      0.16      0.23        45\n",
      "Sexual Orientation       0.60      0.25      0.35        36\n",
      "         Political       0.73      0.60      0.66       106\n",
      "\n",
      "         micro avg       0.67      0.31      0.42       716\n",
      "         macro avg       0.38      0.21      0.26       716\n",
      "      weighted avg       0.52      0.31      0.38       716\n",
      "       samples avg       0.40      0.29      0.32       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/55, Training Loss: 0.2561719933199504\n",
      "Epoch 8/55, Training Loss: 0.24840444917716678\n",
      "Validation Loss: 0.30620233342051506\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.42      0.08      0.13        64\n",
      "         Behaviour       0.62      0.56      0.59       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.50      0.03      0.05        39\n",
      "          Religion       0.66      0.33      0.44        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.52      0.39      0.45       110\n",
      "            Gender       0.50      0.18      0.26        45\n",
      "Sexual Orientation       0.75      0.17      0.27        36\n",
      "         Political       0.68      0.60      0.64       106\n",
      "\n",
      "         micro avg       0.61      0.37      0.46       716\n",
      "         macro avg       0.46      0.23      0.28       716\n",
      "      weighted avg       0.56      0.37      0.42       716\n",
      "       samples avg       0.44      0.33      0.36       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/55, Training Loss: 0.2354184625640748\n",
      "Epoch 10/55, Training Loss: 0.22159323947770254\n",
      "Validation Loss: 0.31245482712984085\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.38      0.08      0.13        64\n",
      "         Behaviour       0.68      0.49      0.57       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.50      0.10      0.17        39\n",
      "          Religion       0.64      0.53      0.58        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.60      0.35      0.44       110\n",
      "            Gender       0.54      0.16      0.24        45\n",
      "Sexual Orientation       0.62      0.36      0.46        36\n",
      "         Political       0.64      0.68      0.66       106\n",
      "\n",
      "         micro avg       0.63      0.39      0.48       716\n",
      "         macro avg       0.46      0.27      0.32       716\n",
      "      weighted avg       0.57      0.39      0.44       716\n",
      "       samples avg       0.43      0.34      0.36       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/55, Training Loss: 0.205495275438778\n",
      "Epoch 12/55, Training Loss: 0.1927917001266328\n",
      "Validation Loss: 0.32752672769129276\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.41      0.14      0.21        64\n",
      "         Behaviour       0.63      0.58      0.61       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.44      0.10      0.17        39\n",
      "          Religion       0.64      0.54      0.59        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.58      0.35      0.44       110\n",
      "            Gender       0.65      0.24      0.35        45\n",
      "Sexual Orientation       0.60      0.50      0.55        36\n",
      "         Political       0.68      0.49      0.57       106\n",
      "\n",
      "         micro avg       0.62      0.41      0.49       716\n",
      "         macro avg       0.46      0.30      0.35       716\n",
      "      weighted avg       0.57      0.41      0.46       716\n",
      "       samples avg       0.42      0.35      0.36       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/55, Training Loss: 0.18086657613988907\n",
      "Epoch 14/55, Training Loss: 0.16603801837043156\n",
      "Validation Loss: 0.3492587376385927\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.38      0.19      0.25        64\n",
      "         Behaviour       0.63      0.56      0.59       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.38      0.13      0.19        39\n",
      "          Religion       0.58      0.54      0.56        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.51      0.47      0.49       110\n",
      "            Gender       0.55      0.40      0.46        45\n",
      "Sexual Orientation       0.61      0.47      0.53        36\n",
      "         Political       0.67      0.61      0.64       106\n",
      "\n",
      "         micro avg       0.58      0.45      0.51       716\n",
      "         macro avg       0.43      0.34      0.37       716\n",
      "      weighted avg       0.53      0.45      0.48       716\n",
      "       samples avg       0.46      0.39      0.40       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/55, Training Loss: 0.15606361651231374\n",
      "Epoch 16/55, Training Loss: 0.14345517636291563\n",
      "Validation Loss: 0.3744709473103285\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.44      0.12      0.20        64\n",
      "         Behaviour       0.59      0.57      0.58       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.38      0.13      0.19        39\n",
      "          Religion       0.59      0.60      0.60        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.57      0.25      0.34       110\n",
      "            Gender       0.52      0.29      0.37        45\n",
      "Sexual Orientation       0.65      0.42      0.51        36\n",
      "         Political       0.66      0.53      0.59       106\n",
      "\n",
      "         micro avg       0.59      0.40      0.47       716\n",
      "         macro avg       0.44      0.29      0.34       716\n",
      "      weighted avg       0.54      0.40      0.44       716\n",
      "       samples avg       0.40      0.35      0.35       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/55, Training Loss: 0.12908654908339182\n",
      "Epoch 18/55, Training Loss: 0.12055867993169361\n",
      "Validation Loss: 0.3849548101425171\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.42      0.08      0.13        64\n",
      "         Behaviour       0.59      0.62      0.60       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.25      0.05      0.09        39\n",
      "          Religion       0.64      0.57      0.60        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.49      0.27      0.35       110\n",
      "            Gender       0.44      0.24      0.31        45\n",
      "Sexual Orientation       0.60      0.42      0.49        36\n",
      "         Political       0.67      0.62      0.65       106\n",
      "\n",
      "         micro avg       0.58      0.41      0.48       716\n",
      "         macro avg       0.41      0.29      0.32       716\n",
      "      weighted avg       0.52      0.41      0.44       716\n",
      "       samples avg       0.45      0.37      0.38       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/55, Training Loss: 0.11019885788361232\n",
      "Epoch 20/55, Training Loss: 0.10439523356774497\n",
      "Validation Loss: 0.4090309012681246\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.36      0.06      0.11        64\n",
      "         Behaviour       0.57      0.62      0.60       194\n",
      "          Physical       0.00      0.00      0.00        30\n",
      "             Class       0.33      0.08      0.12        39\n",
      "          Religion       0.59      0.63      0.61        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.55      0.27      0.36       110\n",
      "            Gender       0.40      0.40      0.40        45\n",
      "Sexual Orientation       0.56      0.50      0.53        36\n",
      "         Political       0.67      0.66      0.66       106\n",
      "\n",
      "         micro avg       0.57      0.44      0.49       716\n",
      "         macro avg       0.40      0.32      0.34       716\n",
      "      weighted avg       0.51      0.44      0.45       716\n",
      "       samples avg       0.47      0.40      0.40       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/55, Training Loss: 0.09278861320917568\n",
      "Epoch 22/55, Training Loss: 0.08155515405630308\n",
      "Validation Loss: 0.41994452476501465\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Race       0.44      0.11      0.17        64\n",
      "         Behaviour       0.61      0.53      0.57       194\n",
      "          Physical       0.12      0.03      0.05        30\n",
      "             Class       0.50      0.05      0.09        39\n",
      "          Religion       0.64      0.40      0.49        81\n",
      "        Disability       0.00      0.00      0.00        11\n",
      "         Ethnicity       0.55      0.34      0.42       110\n",
      "            Gender       0.52      0.27      0.35        45\n",
      "Sexual Orientation       0.52      0.44      0.48        36\n",
      "         Political       0.65      0.61      0.63       106\n",
      "\n",
      "         micro avg       0.59      0.38      0.46       716\n",
      "         macro avg       0.46      0.28      0.33       716\n",
      "      weighted avg       0.55      0.38      0.44       716\n",
      "       samples avg       0.42      0.35      0.36       716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kafi/miniforge3/envs/voice/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/55, Training Loss: 0.07507275466969798\n",
      "Epoch 24/55, Training Loss: 0.06317263313879569\n",
      "Validation Loss: 0.43767437525093555\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state[:, 0, :]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        return self.classifier(sequence_output)\n",
    "\n",
    "def evaluate_model(model, dataloader, label_names, device):\n",
    "    \"\"\"Evaluate the model on the validation set\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_predictions = torch.cat(all_predictions, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    # Convert predictions to binary (0 or 1)\n",
    "    final_predictions = (all_predictions > 0.5).astype(int)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, final_predictions, target_names=label_names))\n",
    "    return all_predictions, all_labels\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, label_columns, epochs=10):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set up training parameters\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    best_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % 2 == 0:  # Evaluate every 2 epochs\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            print(f\"Validation Loss: {avg_val_loss}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "            \n",
    "            # Evaluate model performance\n",
    "            evaluate_model(model, val_dataloader, label_columns, device)\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    data = pd.read_csv('./Bengali Heatspeech dataset.csv').dropna()\n",
    "    label_columns = ['Race', 'Behaviour', 'Physical', 'Class', 'Religion', \n",
    "                    'Disability', 'Ethnicity', 'Gender', 'Sexual Orientation', 'Political']\n",
    "    \n",
    "    # Tokenize texts\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    encoded_data = tokenizer(\n",
    "        list(data['Text']),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Prepare labels\n",
    "    labels = torch.tensor(data[label_columns].values, dtype=torch.float32)\n",
    "    \n",
    "    # Split data\n",
    "    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "        encoded_data['input_ids'], \n",
    "        encoded_data['attention_mask'],\n",
    "        labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = MultiLabelClassifier(num_labels=len(label_columns))\n",
    "    train_model(model, train_dataloader, val_dataloader, label_columns, epochs=55)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
